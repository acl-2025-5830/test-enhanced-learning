{
    "results": {
        "headqa_en": {
            "alias": "headqa_en",
            "acc,none": 0.25200583515681985,
            "acc_stderr,none": 0.008292775065040616,
            "acc_norm,none": 0.2990517870167761,
            "acc_norm_stderr,none": 0.008745036966349156
        },
        "medmcqa_continuation": {
            "alias": "medmcqa_continuation",
            "acc,none": 0.22448003825005977,
            "acc_stderr,none": 0.0064519848668805726
        },
        "medqa_4options_continuation": {
            "alias": "medqa_4options_continuation",
            "acc,none": 0.25687352710133543,
            "acc_stderr,none": 0.012250321462084827
        },
        "mmlu_continuation_anatomy": {
            "alias": "mmlu_continuation_anatomy",
            "acc,none": 0.31851851851851853,
            "acc_stderr,none": 0.0402477840197711,
            "acc_norm,none": 0.28888888888888886,
            "acc_norm_stderr,none": 0.0391545063041425
        },
        "mmlu_continuation_clinical_knowledge": {
            "alias": "mmlu_continuation_clinical_knowledge",
            "acc,none": 0.24150943396226415,
            "acc_stderr,none": 0.026341480371118352,
            "acc_norm,none": 0.30943396226415093,
            "acc_norm_stderr,none": 0.028450154794118627
        },
        "mmlu_continuation_college_biology": {
            "alias": "mmlu_continuation_college_biology",
            "acc,none": 0.3055555555555556,
            "acc_stderr,none": 0.03852084696008534,
            "acc_norm,none": 0.2847222222222222,
            "acc_norm_stderr,none": 0.03773809990686936
        },
        "mmlu_continuation_college_medicine": {
            "alias": "mmlu_continuation_college_medicine",
            "acc,none": 0.3583815028901734,
            "acc_stderr,none": 0.036563436533531585,
            "acc_norm,none": 0.28901734104046245,
            "acc_norm_stderr,none": 0.034564257450869995
        },
        "mmlu_continuation_medical_genetics": {
            "alias": "mmlu_continuation_medical_genetics",
            "acc,none": 0.25,
            "acc_stderr,none": 0.04351941398892446,
            "acc_norm,none": 0.3,
            "acc_norm_stderr,none": 0.046056618647183814
        },
        "mmlu_continuation_professional_medicine": {
            "alias": "mmlu_continuation_professional_medicine",
            "acc,none": 0.2647058823529412,
            "acc_stderr,none": 0.026799562024887674,
            "acc_norm,none": 0.2867647058823529,
            "acc_norm_stderr,none": 0.027472274473233818
        }
    },
    "group_subtasks": {
        "headqa_en": [],
        "medmcqa_continuation": [],
        "medqa_4options_continuation": [],
        "mmlu_continuation_anatomy": [],
        "mmlu_continuation_clinical_knowledge": [],
        "mmlu_continuation_college_biology": [],
        "mmlu_continuation_college_medicine": [],
        "mmlu_continuation_medical_genetics": [],
        "mmlu_continuation_professional_medicine": []
    },
    "configs": {
        "headqa_en": {
            "task": "headqa_en",
            "tag": "headqa",
            "dataset_path": "EleutherAI/headqa",
            "dataset_name": "en",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{qtext}}\nAnswer:",
            "doc_to_target": "{{ra - 1}}",
            "unsafe_code": false,
            "doc_to_choice": "{{answers|map(attribute='atext')|list}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "query",
            "metadata": {
                "version": 1.0
            }
        },
        "medmcqa_continuation": {
            "task": "medmcqa_continuation",
            "dataset_path": "kotmul/preprocessed_medmcqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "validation",
            "doc_to_text": "Question: {{question}}\nAnswer: ",
            "doc_to_target": "{{cop}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true
        },
        "medqa_4options_continuation": {
            "task": "medqa_4options_continuation",
            "dataset_path": "kotmul/preprocessed_medqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{sent1}}\nAnswer: ",
            "doc_to_target": "{{label}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean"
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false
        },
        "mmlu_continuation_anatomy": {
            "task": "mmlu_continuation_anatomy",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_clinical_knowledge": {
            "task": "mmlu_continuation_clinical_knowledge",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_biology": {
            "task": "mmlu_continuation_college_biology",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_medicine": {
            "task": "mmlu_continuation_college_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_medical_genetics": {
            "task": "mmlu_continuation_medical_genetics",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_professional_medicine": {
            "task": "mmlu_continuation_professional_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "headqa_en": 1.0,
        "medmcqa_continuation": "Yaml",
        "medqa_4options_continuation": "Yaml",
        "mmlu_continuation_anatomy": 1.0,
        "mmlu_continuation_clinical_knowledge": 1.0,
        "mmlu_continuation_college_biology": 1.0,
        "mmlu_continuation_college_medicine": 1.0,
        "mmlu_continuation_medical_genetics": 1.0,
        "mmlu_continuation_professional_medicine": 1.0
    },
    "n-shot": {
        "headqa_en": 4,
        "medmcqa_continuation": 4,
        "medqa_4options_continuation": 4,
        "mmlu_continuation_anatomy": 4,
        "mmlu_continuation_clinical_knowledge": 4,
        "mmlu_continuation_college_biology": 4,
        "mmlu_continuation_college_medicine": 4,
        "mmlu_continuation_medical_genetics": 4,
        "mmlu_continuation_professional_medicine": 4
    },
    "higher_is_better": {
        "headqa_en": {
            "acc": true,
            "acc_norm": true
        },
        "medmcqa_continuation": {
            "acc": true
        },
        "medqa_4options_continuation": {
            "acc": true
        },
        "mmlu_continuation_anatomy": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_clinical_knowledge": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_biology": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_medicine": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_medical_genetics": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_professional_medicine": {
            "acc": true,
            "acc_norm": true
        }
    },
    "n-samples": {
        "mmlu_continuation_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_continuation_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_continuation_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_continuation_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_continuation_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_continuation_anatomy": {
            "original": 135,
            "effective": 135
        },
        "medqa_4options_continuation": {
            "original": 1273,
            "effective": 1273
        },
        "medmcqa_continuation": {
            "original": 4183,
            "effective": 4183
        },
        "headqa_en": {
            "original": 2742,
            "effective": 2742
        }
    },
    "config": {
        "model": "hf",
        "model_num_parameters": 1557611200,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 42,
        "numpy_seed": 42,
        "torch_seed": 42,
        "fewshot_seed": 42
    },
    "git_hash": "370e2f9e",
    "date": 1739537851.0042503,
    "transformers_version": "4.48.1",
    "upper_git_hash": "370e2f9e5bbe59912644b1b6e052e17be31d6858",
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "50256"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "50256"
    ],
    "tokenizer_bos_token": [
        "<|endoftext|>",
        "50256"
    ],
    "eot_token_id": 50256,
    "max_length": 1024,
    "task_hashes": {},
    "model_source": "hf",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 778177.44251302,
    "end_time": 778311.138928474,
    "total_evaluation_time_seconds": "133.69641545403283"
}