{
    "results": {
        "headqa_en": {
            "alias": "headqa_en",
            "acc,none": 0.2337709700948213,
            "acc_stderr,none": 0.008083883995835853,
            "acc_norm,none": 0.27935813274981763,
            "acc_norm_stderr,none": 0.008570099944976721
        },
        "medmcqa_continuation": {
            "alias": "medmcqa_continuation",
            "acc,none": 0.2223284723882381,
            "acc_stderr,none": 0.006429891241369222
        },
        "medqa_4options_continuation": {
            "alias": "medqa_4options_continuation",
            "acc,none": 0.2356637863315004,
            "acc_stderr,none": 0.011899948672772743
        },
        "mmlu_continuation_anatomy": {
            "alias": "mmlu_continuation_anatomy",
            "acc,none": 0.31851851851851853,
            "acc_stderr,none": 0.04024778401977111,
            "acc_norm,none": 0.3111111111111111,
            "acc_norm_stderr,none": 0.03999262876617723
        },
        "mmlu_continuation_clinical_knowledge": {
            "alias": "mmlu_continuation_clinical_knowledge",
            "acc,none": 0.23773584905660378,
            "acc_stderr,none": 0.026199808807561932,
            "acc_norm,none": 0.29056603773584905,
            "acc_norm_stderr,none": 0.027943219989337135
        },
        "mmlu_continuation_college_biology": {
            "alias": "mmlu_continuation_college_biology",
            "acc,none": 0.2569444444444444,
            "acc_stderr,none": 0.03653946969442099,
            "acc_norm,none": 0.3194444444444444,
            "acc_norm_stderr,none": 0.03899073687357336
        },
        "mmlu_continuation_college_medicine": {
            "alias": "mmlu_continuation_college_medicine",
            "acc,none": 0.2774566473988439,
            "acc_stderr,none": 0.03414014007044037,
            "acc_norm,none": 0.27167630057803466,
            "acc_norm_stderr,none": 0.03391750322321659
        },
        "mmlu_continuation_medical_genetics": {
            "alias": "mmlu_continuation_medical_genetics",
            "acc,none": 0.3,
            "acc_stderr,none": 0.046056618647183814,
            "acc_norm,none": 0.3,
            "acc_norm_stderr,none": 0.046056618647183814
        },
        "mmlu_continuation_professional_medicine": {
            "alias": "mmlu_continuation_professional_medicine",
            "acc,none": 0.25,
            "acc_stderr,none": 0.026303648393696036,
            "acc_norm,none": 0.2610294117647059,
            "acc_norm_stderr,none": 0.02667925227010313
        }
    },
    "group_subtasks": {
        "headqa_en": [],
        "medmcqa_continuation": [],
        "medqa_4options_continuation": [],
        "mmlu_continuation_anatomy": [],
        "mmlu_continuation_clinical_knowledge": [],
        "mmlu_continuation_college_biology": [],
        "mmlu_continuation_college_medicine": [],
        "mmlu_continuation_medical_genetics": [],
        "mmlu_continuation_professional_medicine": []
    },
    "configs": {
        "headqa_en": {
            "task": "headqa_en",
            "tag": "headqa",
            "dataset_path": "EleutherAI/headqa",
            "dataset_name": "en",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{qtext}}\nAnswer:",
            "doc_to_target": "{{ra - 1}}",
            "unsafe_code": false,
            "doc_to_choice": "{{answers|map(attribute='atext')|list}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "query",
            "metadata": {
                "version": 1.0
            }
        },
        "medmcqa_continuation": {
            "task": "medmcqa_continuation",
            "dataset_path": "kotmul/preprocessed_medmcqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "validation",
            "doc_to_text": "Question: {{question}}\nAnswer: ",
            "doc_to_target": "{{cop}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true
        },
        "medqa_4options_continuation": {
            "task": "medqa_4options_continuation",
            "dataset_path": "kotmul/preprocessed_medqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{sent1}}\nAnswer: ",
            "doc_to_target": "{{label}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean"
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false
        },
        "mmlu_continuation_anatomy": {
            "task": "mmlu_continuation_anatomy",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_clinical_knowledge": {
            "task": "mmlu_continuation_clinical_knowledge",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_biology": {
            "task": "mmlu_continuation_college_biology",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_medicine": {
            "task": "mmlu_continuation_college_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_medical_genetics": {
            "task": "mmlu_continuation_medical_genetics",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_professional_medicine": {
            "task": "mmlu_continuation_professional_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "headqa_en": 1.0,
        "medmcqa_continuation": "Yaml",
        "medqa_4options_continuation": "Yaml",
        "mmlu_continuation_anatomy": 1.0,
        "mmlu_continuation_clinical_knowledge": 1.0,
        "mmlu_continuation_college_biology": 1.0,
        "mmlu_continuation_college_medicine": 1.0,
        "mmlu_continuation_medical_genetics": 1.0,
        "mmlu_continuation_professional_medicine": 1.0
    },
    "n-shot": {
        "headqa_en": 4,
        "medmcqa_continuation": 4,
        "medqa_4options_continuation": 4,
        "mmlu_continuation_anatomy": 4,
        "mmlu_continuation_clinical_knowledge": 4,
        "mmlu_continuation_college_biology": 4,
        "mmlu_continuation_college_medicine": 4,
        "mmlu_continuation_medical_genetics": 4,
        "mmlu_continuation_professional_medicine": 4
    },
    "higher_is_better": {
        "headqa_en": {
            "acc": true,
            "acc_norm": true
        },
        "medmcqa_continuation": {
            "acc": true
        },
        "medqa_4options_continuation": {
            "acc": true
        },
        "mmlu_continuation_anatomy": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_clinical_knowledge": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_biology": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_medicine": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_medical_genetics": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_professional_medicine": {
            "acc": true,
            "acc_norm": true
        }
    },
    "n-samples": {
        "mmlu_continuation_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_continuation_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_continuation_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_continuation_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_continuation_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_continuation_anatomy": {
            "original": 135,
            "effective": 135
        },
        "medqa_4options_continuation": {
            "original": 1273,
            "effective": 1273
        },
        "medmcqa_continuation": {
            "original": 4183,
            "effective": 4183
        },
        "headqa_en": {
            "original": 2742,
            "effective": 2742
        }
    },
    "config": {
        "model": "hf",
        "model_num_parameters": 354823168,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 42,
        "numpy_seed": 42,
        "torch_seed": 42,
        "fewshot_seed": 42
    },
    "git_hash": "370e2f9e",
    "date": 1739537435.1278832,
    "transformers_version": "4.48.1",
    "upper_git_hash": "370e2f9e5bbe59912644b1b6e052e17be31d6858",
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "50256"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "50256"
    ],
    "tokenizer_bos_token": [
        "<|endoftext|>",
        "50256"
    ],
    "eot_token_id": 50256,
    "max_length": 1024,
    "task_hashes": {},
    "model_source": "hf",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 777761.588860689,
    "end_time": 777866.365303082,
    "total_evaluation_time_seconds": "104.77644239307847"
}