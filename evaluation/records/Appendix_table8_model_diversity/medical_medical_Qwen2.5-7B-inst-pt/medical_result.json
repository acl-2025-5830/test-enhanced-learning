{
    "results": {
        "headqa_en": {
            "alias": "headqa_en",
            "acc,none": 0.3785557986870897,
            "acc_stderr,none": 0.009264276399596566,
            "acc_norm,none": 0.43216630196936545,
            "acc_norm_stderr,none": 0.009461968834126427
        },
        "medmcqa_continuation": {
            "alias": "medmcqa_continuation",
            "acc,none": 0.3361224001912503,
            "acc_stderr,none": 0.00730467202879973
        },
        "medqa_4options_continuation": {
            "alias": "medqa_4options_continuation",
            "acc,none": 0.4218381775333857,
            "acc_stderr,none": 0.013846948938187617
        },
        "mmlu_continuation_anatomy": {
            "alias": "mmlu_continuation_anatomy",
            "acc,none": 0.5407407407407407,
            "acc_stderr,none": 0.04304979692464242,
            "acc_norm,none": 0.5259259259259259,
            "acc_norm_stderr,none": 0.04313531696750575
        },
        "mmlu_continuation_clinical_knowledge": {
            "alias": "mmlu_continuation_clinical_knowledge",
            "acc,none": 0.4226415094339623,
            "acc_stderr,none": 0.03040233144576954,
            "acc_norm,none": 0.5320754716981132,
            "acc_norm_stderr,none": 0.030709486992556545
        },
        "mmlu_continuation_college_biology": {
            "alias": "mmlu_continuation_college_biology",
            "acc,none": 0.5416666666666666,
            "acc_stderr,none": 0.04166666666666665,
            "acc_norm,none": 0.5763888888888888,
            "acc_norm_stderr,none": 0.041321250197233685
        },
        "mmlu_continuation_college_medicine": {
            "alias": "mmlu_continuation_college_medicine",
            "acc,none": 0.41040462427745666,
            "acc_stderr,none": 0.03750757044895536,
            "acc_norm,none": 0.4508670520231214,
            "acc_norm_stderr,none": 0.03794012674697029
        },
        "mmlu_continuation_medical_genetics": {
            "alias": "mmlu_continuation_medical_genetics",
            "acc,none": 0.49,
            "acc_stderr,none": 0.05024183937956912,
            "acc_norm,none": 0.58,
            "acc_norm_stderr,none": 0.049604496374885836
        },
        "mmlu_continuation_professional_medicine": {
            "alias": "mmlu_continuation_professional_medicine",
            "acc,none": 0.49264705882352944,
            "acc_stderr,none": 0.030369552523902173,
            "acc_norm,none": 0.5110294117647058,
            "acc_norm_stderr,none": 0.030365446477275675
        }
    },
    "group_subtasks": {
        "headqa_en": [],
        "medmcqa_continuation": [],
        "medqa_4options_continuation": [],
        "mmlu_continuation_anatomy": [],
        "mmlu_continuation_clinical_knowledge": [],
        "mmlu_continuation_college_biology": [],
        "mmlu_continuation_college_medicine": [],
        "mmlu_continuation_medical_genetics": [],
        "mmlu_continuation_professional_medicine": []
    },
    "configs": {
        "headqa_en": {
            "task": "headqa_en",
            "tag": "headqa",
            "dataset_path": "EleutherAI/headqa",
            "dataset_name": "en",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{qtext}}\nAnswer:",
            "doc_to_target": "{{ra - 1}}",
            "unsafe_code": false,
            "doc_to_choice": "{{answers|map(attribute='atext')|list}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "query",
            "metadata": {
                "version": 1.0
            }
        },
        "medmcqa_continuation": {
            "task": "medmcqa_continuation",
            "dataset_path": "kotmul/preprocessed_medmcqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "validation",
            "doc_to_text": "Question: {{question}}\nAnswer: ",
            "doc_to_target": "{{cop}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true
        },
        "medqa_4options_continuation": {
            "task": "medqa_4options_continuation",
            "dataset_path": "kotmul/preprocessed_medqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{sent1}}\nAnswer: ",
            "doc_to_target": "{{label}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean"
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false
        },
        "mmlu_continuation_anatomy": {
            "task": "mmlu_continuation_anatomy",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_clinical_knowledge": {
            "task": "mmlu_continuation_clinical_knowledge",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_biology": {
            "task": "mmlu_continuation_college_biology",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_medicine": {
            "task": "mmlu_continuation_college_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_medical_genetics": {
            "task": "mmlu_continuation_medical_genetics",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_professional_medicine": {
            "task": "mmlu_continuation_professional_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "headqa_en": 1.0,
        "medmcqa_continuation": "Yaml",
        "medqa_4options_continuation": "Yaml",
        "mmlu_continuation_anatomy": 1.0,
        "mmlu_continuation_clinical_knowledge": 1.0,
        "mmlu_continuation_college_biology": 1.0,
        "mmlu_continuation_college_medicine": 1.0,
        "mmlu_continuation_medical_genetics": 1.0,
        "mmlu_continuation_professional_medicine": 1.0
    },
    "n-shot": {
        "headqa_en": 4,
        "medmcqa_continuation": 4,
        "medqa_4options_continuation": 4,
        "mmlu_continuation_anatomy": 4,
        "mmlu_continuation_clinical_knowledge": 4,
        "mmlu_continuation_college_biology": 4,
        "mmlu_continuation_college_medicine": 4,
        "mmlu_continuation_medical_genetics": 4,
        "mmlu_continuation_professional_medicine": 4
    },
    "higher_is_better": {
        "headqa_en": {
            "acc": true,
            "acc_norm": true
        },
        "medmcqa_continuation": {
            "acc": true
        },
        "medqa_4options_continuation": {
            "acc": true
        },
        "mmlu_continuation_anatomy": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_clinical_knowledge": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_biology": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_medicine": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_medical_genetics": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_professional_medicine": {
            "acc": true,
            "acc_norm": true
        }
    },
    "n-samples": {
        "mmlu_continuation_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_continuation_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_continuation_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_continuation_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_continuation_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_continuation_anatomy": {
            "original": 135,
            "effective": 135
        },
        "medqa_4options_continuation": {
            "original": 1273,
            "effective": 1273
        },
        "medmcqa_continuation": {
            "original": 4183,
            "effective": 4183
        },
        "headqa_en": {
            "original": 2742,
            "effective": 2742
        }
    },
    "config": {
        "model": "hf",
        "model_num_parameters": 7615616512,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 42,
        "numpy_seed": 42,
        "torch_seed": 42,
        "fewshot_seed": 42
    },
    "git_hash": "370e2f9e",
    "date": 1739537014.8923147,
    "transformers_version": "4.48.1",
    "upper_git_hash": "370e2f9e5bbe59912644b1b6e052e17be31d6858",
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "151643"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "151643"
    ],
    "tokenizer_bos_token": [
        null,
        "None"
    ],
    "eot_token_id": 151643,
    "max_length": 131072,
    "task_hashes": {},
    "model_source": "hf",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 777341.486467676,
    "end_time": 777508.158837794,
    "total_evaluation_time_seconds": "166.6723701179726"
}