{
    "results": {
        "headqa_en": {
            "alias": "headqa_en",
            "acc,none": 0.3085339168490153,
            "acc_stderr,none": 0.008822311335958321,
            "acc_norm,none": 0.3533916849015317,
            "acc_norm_stderr,none": 0.009130494266696288
        },
        "medmcqa_continuation": {
            "alias": "medmcqa_continuation",
            "acc,none": 0.25938321778627776,
            "acc_stderr,none": 0.006777596299678443
        },
        "medqa_4options_continuation": {
            "alias": "medqa_4options_continuation",
            "acc,none": 0.2827965435978005,
            "acc_stderr,none": 0.012627421572573405
        },
        "mmlu_continuation_anatomy": {
            "alias": "mmlu_continuation_anatomy",
            "acc,none": 0.37777777777777777,
            "acc_stderr,none": 0.04188307537595853,
            "acc_norm,none": 0.3925925925925926,
            "acc_norm_stderr,none": 0.04218506215368879
        },
        "mmlu_continuation_clinical_knowledge": {
            "alias": "mmlu_continuation_clinical_knowledge",
            "acc,none": 0.3169811320754717,
            "acc_stderr,none": 0.028637235639800925,
            "acc_norm,none": 0.3886792452830189,
            "acc_norm_stderr,none": 0.03000048544867599
        },
        "mmlu_continuation_college_biology": {
            "alias": "mmlu_continuation_college_biology",
            "acc,none": 0.4027777777777778,
            "acc_stderr,none": 0.04101405519842426,
            "acc_norm,none": 0.4166666666666667,
            "acc_norm_stderr,none": 0.041227287076512825
        },
        "mmlu_continuation_college_medicine": {
            "alias": "mmlu_continuation_college_medicine",
            "acc,none": 0.3236994219653179,
            "acc_stderr,none": 0.03567603799639171,
            "acc_norm,none": 0.34104046242774566,
            "acc_norm_stderr,none": 0.036146654241808254
        },
        "mmlu_continuation_medical_genetics": {
            "alias": "mmlu_continuation_medical_genetics",
            "acc,none": 0.29,
            "acc_stderr,none": 0.04560480215720684,
            "acc_norm,none": 0.37,
            "acc_norm_stderr,none": 0.048523658709391
        },
        "mmlu_continuation_professional_medicine": {
            "alias": "mmlu_continuation_professional_medicine",
            "acc,none": 0.33455882352941174,
            "acc_stderr,none": 0.028661996202335307,
            "acc_norm,none": 0.33455882352941174,
            "acc_norm_stderr,none": 0.028661996202335307
        }
    },
    "group_subtasks": {
        "headqa_en": [],
        "medmcqa_continuation": [],
        "medqa_4options_continuation": [],
        "mmlu_continuation_anatomy": [],
        "mmlu_continuation_clinical_knowledge": [],
        "mmlu_continuation_college_biology": [],
        "mmlu_continuation_college_medicine": [],
        "mmlu_continuation_medical_genetics": [],
        "mmlu_continuation_professional_medicine": []
    },
    "configs": {
        "headqa_en": {
            "task": "headqa_en",
            "tag": "headqa",
            "dataset_path": "EleutherAI/headqa",
            "dataset_name": "en",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{qtext}}\nAnswer:",
            "doc_to_target": "{{ra - 1}}",
            "unsafe_code": false,
            "doc_to_choice": "{{answers|map(attribute='atext')|list}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "query",
            "metadata": {
                "version": 1.0
            }
        },
        "medmcqa_continuation": {
            "task": "medmcqa_continuation",
            "dataset_path": "kotmul/preprocessed_medmcqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "validation",
            "doc_to_text": "Question: {{question}}\nAnswer: ",
            "doc_to_target": "{{cop}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true
        },
        "medqa_4options_continuation": {
            "task": "medqa_4options_continuation",
            "dataset_path": "kotmul/preprocessed_medqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{sent1}}\nAnswer: ",
            "doc_to_target": "{{label}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean"
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false
        },
        "mmlu_continuation_anatomy": {
            "task": "mmlu_continuation_anatomy",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_clinical_knowledge": {
            "task": "mmlu_continuation_clinical_knowledge",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_biology": {
            "task": "mmlu_continuation_college_biology",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_medicine": {
            "task": "mmlu_continuation_college_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_medical_genetics": {
            "task": "mmlu_continuation_medical_genetics",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_professional_medicine": {
            "task": "mmlu_continuation_professional_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "headqa_en": 1.0,
        "medmcqa_continuation": "Yaml",
        "medqa_4options_continuation": "Yaml",
        "mmlu_continuation_anatomy": 1.0,
        "mmlu_continuation_clinical_knowledge": 1.0,
        "mmlu_continuation_college_biology": 1.0,
        "mmlu_continuation_college_medicine": 1.0,
        "mmlu_continuation_medical_genetics": 1.0,
        "mmlu_continuation_professional_medicine": 1.0
    },
    "n-shot": {
        "headqa_en": 4,
        "medmcqa_continuation": 4,
        "medqa_4options_continuation": 4,
        "mmlu_continuation_anatomy": 4,
        "mmlu_continuation_clinical_knowledge": 4,
        "mmlu_continuation_college_biology": 4,
        "mmlu_continuation_college_medicine": 4,
        "mmlu_continuation_medical_genetics": 4,
        "mmlu_continuation_professional_medicine": 4
    },
    "higher_is_better": {
        "headqa_en": {
            "acc": true,
            "acc_norm": true
        },
        "medmcqa_continuation": {
            "acc": true
        },
        "medqa_4options_continuation": {
            "acc": true
        },
        "mmlu_continuation_anatomy": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_clinical_knowledge": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_biology": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_medicine": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_medical_genetics": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_professional_medicine": {
            "acc": true,
            "acc_norm": true
        }
    },
    "n-samples": {
        "mmlu_continuation_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_continuation_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_continuation_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_continuation_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_continuation_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_continuation_anatomy": {
            "original": 135,
            "effective": 135
        },
        "medqa_4options_continuation": {
            "original": 1273,
            "effective": 1273
        },
        "medmcqa_continuation": {
            "original": 4183,
            "effective": 4183
        },
        "headqa_en": {
            "original": 2742,
            "effective": 2742
        }
    },
    "config": {
        "model": "hf",
        "model_num_parameters": 6888095744,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 42,
        "numpy_seed": 42,
        "torch_seed": 42,
        "fewshot_seed": 42
    },
    "git_hash": "370e2f9e",
    "date": 1739534947.0736585,
    "transformers_version": "4.48.1",
    "upper_git_hash": "370e2f9e5bbe59912644b1b6e052e17be31d6858",
    "tokenizer_pad_token": [
        "<|padding|>",
        "1"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "50279"
    ],
    "tokenizer_bos_token": [
        null,
        "None"
    ],
    "eot_token_id": 50279,
    "max_length": 2048,
    "task_hashes": {},
    "model_source": "hf",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 775273.634397722,
    "end_time": 775427.728060692,
    "total_evaluation_time_seconds": "154.09366297000088"
}