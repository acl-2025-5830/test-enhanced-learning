{
    "results": {
        "headqa_en": {
            "alias": "headqa_en",
            "acc,none": 0.41611962071480674,
            "acc_stderr,none": 0.009414917525232068,
            "acc_norm,none": 0.47921225382932164,
            "acc_norm_stderr,none": 0.009542008917829525
        },
        "medmcqa_continuation": {
            "alias": "medmcqa_continuation",
            "acc,none": 0.34664116662682287,
            "acc_stderr,none": 0.007359087015336127
        },
        "medqa_4options_continuation": {
            "alias": "medqa_4options_continuation",
            "acc,none": 0.4304791830322074,
            "acc_stderr,none": 0.013883127853625046
        },
        "mmlu_continuation_anatomy": {
            "alias": "mmlu_continuation_anatomy",
            "acc,none": 0.5703703703703704,
            "acc_stderr,none": 0.042763494943765995,
            "acc_norm,none": 0.5407407407407407,
            "acc_norm_stderr,none": 0.04304979692464242
        },
        "mmlu_continuation_clinical_knowledge": {
            "alias": "mmlu_continuation_clinical_knowledge",
            "acc,none": 0.44150943396226416,
            "acc_stderr,none": 0.03056159042673184,
            "acc_norm,none": 0.5283018867924528,
            "acc_norm_stderr,none": 0.030723535249006107
        },
        "mmlu_continuation_college_biology": {
            "alias": "mmlu_continuation_college_biology",
            "acc,none": 0.5625,
            "acc_stderr,none": 0.04148415739394154,
            "acc_norm,none": 0.5555555555555556,
            "acc_norm_stderr,none": 0.04155319955593147
        },
        "mmlu_continuation_college_medicine": {
            "alias": "mmlu_continuation_college_medicine",
            "acc,none": 0.4393063583815029,
            "acc_stderr,none": 0.037842719328874674,
            "acc_norm,none": 0.5028901734104047,
            "acc_norm_stderr,none": 0.038124005659748335
        },
        "mmlu_continuation_medical_genetics": {
            "alias": "mmlu_continuation_medical_genetics",
            "acc,none": 0.51,
            "acc_stderr,none": 0.05024183937956912,
            "acc_norm,none": 0.59,
            "acc_norm_stderr,none": 0.049431107042371025
        },
        "mmlu_continuation_professional_medicine": {
            "alias": "mmlu_continuation_professional_medicine",
            "acc,none": 0.5294117647058824,
            "acc_stderr,none": 0.03032024326500413,
            "acc_norm,none": 0.5330882352941176,
            "acc_norm_stderr,none": 0.030306257722468317
        }
    },
    "group_subtasks": {
        "headqa_en": [],
        "medmcqa_continuation": [],
        "medqa_4options_continuation": [],
        "mmlu_continuation_anatomy": [],
        "mmlu_continuation_clinical_knowledge": [],
        "mmlu_continuation_college_biology": [],
        "mmlu_continuation_college_medicine": [],
        "mmlu_continuation_medical_genetics": [],
        "mmlu_continuation_professional_medicine": []
    },
    "configs": {
        "headqa_en": {
            "task": "headqa_en",
            "tag": "headqa",
            "dataset_path": "EleutherAI/headqa",
            "dataset_name": "en",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{qtext}}\nAnswer:",
            "doc_to_target": "{{ra - 1}}",
            "unsafe_code": false,
            "doc_to_choice": "{{answers|map(attribute='atext')|list}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "query",
            "metadata": {
                "version": 1.0
            }
        },
        "medmcqa_continuation": {
            "task": "medmcqa_continuation",
            "dataset_path": "kotmul/preprocessed_medmcqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "validation",
            "doc_to_text": "Question: {{question}}\nAnswer: ",
            "doc_to_target": "{{cop}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true
        },
        "medqa_4options_continuation": {
            "task": "medqa_4options_continuation",
            "dataset_path": "kotmul/preprocessed_medqa",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{sent1}}\nAnswer: ",
            "doc_to_target": "{{label}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 4,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean"
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false
        },
        "mmlu_continuation_anatomy": {
            "task": "mmlu_continuation_anatomy",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_clinical_knowledge": {
            "task": "mmlu_continuation_clinical_knowledge",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_biology": {
            "task": "mmlu_continuation_college_biology",
            "tag": "mmlu_continuation_stem",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_college_medicine": {
            "task": "mmlu_continuation_college_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_medical_genetics": {
            "task": "mmlu_continuation_medical_genetics",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_continuation_professional_medicine": {
            "task": "mmlu_continuation_professional_medicine",
            "tag": "mmlu_continuation_other",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "unsafe_code": false,
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 4,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "headqa_en": 1.0,
        "medmcqa_continuation": "Yaml",
        "medqa_4options_continuation": "Yaml",
        "mmlu_continuation_anatomy": 1.0,
        "mmlu_continuation_clinical_knowledge": 1.0,
        "mmlu_continuation_college_biology": 1.0,
        "mmlu_continuation_college_medicine": 1.0,
        "mmlu_continuation_medical_genetics": 1.0,
        "mmlu_continuation_professional_medicine": 1.0
    },
    "n-shot": {
        "headqa_en": 4,
        "medmcqa_continuation": 4,
        "medqa_4options_continuation": 4,
        "mmlu_continuation_anatomy": 4,
        "mmlu_continuation_clinical_knowledge": 4,
        "mmlu_continuation_college_biology": 4,
        "mmlu_continuation_college_medicine": 4,
        "mmlu_continuation_medical_genetics": 4,
        "mmlu_continuation_professional_medicine": 4
    },
    "higher_is_better": {
        "headqa_en": {
            "acc": true,
            "acc_norm": true
        },
        "medmcqa_continuation": {
            "acc": true
        },
        "medqa_4options_continuation": {
            "acc": true
        },
        "mmlu_continuation_anatomy": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_clinical_knowledge": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_biology": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_college_medicine": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_medical_genetics": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation_professional_medicine": {
            "acc": true,
            "acc_norm": true
        }
    },
    "n-samples": {
        "mmlu_continuation_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_continuation_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_continuation_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_continuation_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_continuation_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_continuation_anatomy": {
            "original": 135,
            "effective": 135
        },
        "medqa_4options_continuation": {
            "original": 1273,
            "effective": 1273
        },
        "medmcqa_continuation": {
            "original": 4183,
            "effective": 4183
        },
        "headqa_en": {
            "original": 2742,
            "effective": 2742
        }
    },
    "config": {
        "model": "hf",
        "model_num_parameters": 7615616512,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": "1",
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 42,
        "numpy_seed": 42,
        "torch_seed": 42,
        "fewshot_seed": 42
    },
    "git_hash": "370e2f9e",
    "date": 1739536840.3848593,
    "transformers_version": "4.48.1",
    "upper_git_hash": "370e2f9e5bbe59912644b1b6e052e17be31d6858",
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "151643"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "151643"
    ],
    "tokenizer_bos_token": [
        null,
        "None"
    ],
    "eot_token_id": 151643,
    "max_length": 131072,
    "task_hashes": {},
    "model_source": "hf",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 777166.861265209,
    "end_time": 777329.478758933,
    "total_evaluation_time_seconds": "162.6174937239848"
}